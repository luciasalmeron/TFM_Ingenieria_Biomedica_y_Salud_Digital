{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luciasalmeron/TFM_Ingenieria_Biomedica_y_Salud_Digital/blob/main/ResNet50_con_Albumentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqyJB-pgzLWW"
      },
      "source": [
        "#1. Montaje de Google Drive y Preparación del Entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9SHHRyW0n2T"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Monta Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "C8XAH-3HzQxs"
      },
      "outputs": [],
      "source": [
        "# Instalación de paquetes (solo usar en Colab o Jupyter si es necesario)\n",
        "!pip install split-folders\n",
        "\n",
        "# Procesamiento de imágenes y augmentación\n",
        "import albumentations as A\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# TensorFlow y Keras\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Manejo de archivos\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "!pip install split-folders\n",
        "!pip install keras-tuner\n",
        "\n",
        "# Utilidades\n",
        "import pandas as pd\n",
        "import splitfolders\n",
        "import time\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "\n",
        "\n",
        "!pip install albumentations\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Descompresión del Dataset"
      ],
      "metadata": {
        "id": "SZcnH4fzx8uO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hV2r09WZ1CP7"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/drive/My Drive/imagenes520.zip\" \"/content/imagenes520.zip\"\n",
        "\n",
        "zip_path = \"/content/imagenes520.zip\"  # Ruta del archivo ZIP\n",
        "extract_path = \"/content/imagenes520\"  # Carpeta de destino\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Definición de Variables de Entrenamiento"
      ],
      "metadata": {
        "id": "lPH4J7tzx_u7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y91GZpQp237D"
      },
      "outputs": [],
      "source": [
        "## Training variables ##\n",
        "INPUT_SIZE = 224\n",
        "BATCH_SIZE = 32  # size of the readed batches from generator, must fit on memory\n",
        "VAL_SPLIT = 0.15  # fraction of the images used for validation\n",
        "TEST_SPLIT = 0.15  # fraction of the images used for testing\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Clasificación de Imágenes según Etiquetas"
      ],
      "metadata": {
        "id": "97z52LYtyB_w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sHEC4jvr1KMo"
      },
      "outputs": [],
      "source": [
        "# Leer el archivo Excel\n",
        "df = pd.read_csv(\"/content/drive/My Drive/challenge-training_metadata.csv\", sep=\";\")\n",
        "\n",
        "# Crear las carpetas \"benigno\" y \"maligno\" si no existen\n",
        "BASE_DATASET = '/content/imagenes520/imagenes520/imagenes_clasificadas'   # Carpeta base con las carpetas \"benigno\" y \"maligno\"\n",
        "benigno_folder = BASE_DATASET+'/clasificadas_benigno/'\n",
        "maligno_folder = BASE_DATASET+'/clasificadas_maligno/'\n",
        "\n",
        "SPLITTED_DATASET = 'splitted_dataset'  # Carpeta para los datos divididos\n",
        "SAVE_MODELS_PATH = '/content/drive/My Drive/trained_models'  # Carpeta para guardar los modelos\n",
        "image_folder = '/content/imagenes520/imagenes520'\n",
        "\n",
        "\n",
        "\n",
        "# Crear las carpetas si no existen\n",
        "os.makedirs(benigno_folder, exist_ok=True)\n",
        "os.makedirs(maligno_folder, exist_ok=True)\n",
        "\n",
        "# Contadores\n",
        "benigno_count = 0\n",
        "maligno_count = 0\n",
        "no_encontrado_count = 0\n",
        "\n",
        "# Clasificar las imágenes\n",
        "for index, row in df.iterrows():\n",
        "    image_name = row['isic_id'] + '.jpg'\n",
        "    label = row['diagnosis_1']\n",
        "    image_path = os.path.join(image_folder, image_name)\n",
        "\n",
        "    if os.path.exists(image_path):\n",
        "        if label == 'Benign':\n",
        "            shutil.move(image_path, os.path.join(benigno_folder, image_name))\n",
        "            benigno_count += 1\n",
        "        elif label == 'Malignant':\n",
        "            shutil.move(image_path, os.path.join(maligno_folder, image_name))\n",
        "            maligno_count += 1\n",
        "    else:\n",
        "        #print(f\"Imagen no encontrada: {image_name}\")\n",
        "        no_encontrado_count += 1\n",
        "\n",
        "    # Mostrar progreso en tiempo real\n",
        "    print(f\"Movidas - Benigno: {benigno_count}, Maligno: {maligno_count}, No encontradas: {no_encontrado_count}\")\n",
        "\n",
        "# Resumen final\n",
        "print(\"\\nProceso finalizado.\")\n",
        "print(f\"Total movidas a Benigno: {benigno_count}\")\n",
        "print(f\"Total movidas a Maligno: {maligno_count}\")\n",
        "print(f\"Total imágenes no encontradas: {no_encontrado_count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. División del Dataset en Train/Val/Test"
      ],
      "metadata": {
        "id": "xKAHKDuZyFqi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_etzHR8Y1L24"
      },
      "outputs": [],
      "source": [
        "# Dividir el dataset en entrenamiento, validación y prueba\n",
        "if not os.path.exists(SPLITTED_DATASET):\n",
        "    splitfolders.ratio(BASE_DATASET, output=SPLITTED_DATASET, seed=123, ratio=(1 - VAL_SPLIT - TEST_SPLIT, VAL_SPLIT, TEST_SPLIT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih1bDAcOzXgI"
      },
      "source": [
        "#7. Construcción del Modelo ResNet50 Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxz-uJs3zaLB"
      },
      "outputs": [],
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dense(1, activation='sigmoid')(x)  # Para clasificación binaria\n",
        "model = Model(inputs=base_model.input, outputs=x)\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False  # Congelar las capas preentrenadas de ResNet50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9PVcXlnzcpY"
      },
      "source": [
        "#8. Augmentación con Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huPgWgu0zfjj"
      },
      "outputs": [],
      "source": [
        "transform = A.Compose([\n",
        "    A.RandomCrop(width=256, height=256),\n",
        "    A.HorizontalFlip(),\n",
        "    A.RandomBrightnessContrast(),\n",
        "    A.Rotate(limit=45),\n",
        "    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], always_apply=True)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFVOowHWzmm_"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(256, 256))  # Cambia el tamaño según sea necesario\n",
        "    img_array = image.img_to_array(img)\n",
        "    augmented = transform(image=img_array)\n",
        "    img_array = augmented['image']\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    return img_array\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Conteo de Imágenes por Conjunto"
      ],
      "metadata": {
        "id": "XfQqVaqpyQod"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H59EMfeL3Fsa"
      },
      "outputs": [],
      "source": [
        "def count_images_in_folders(base_path):\n",
        "    folder_counts = {}\n",
        "    for folder in os.listdir(base_path):\n",
        "        folder_path = os.path.join(base_path, folder)\n",
        "        if os.path.isdir(folder_path):\n",
        "            num_images = len([f for f in os.listdir(folder_path) if f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'gif'))])\n",
        "            folder_counts[folder] = num_images\n",
        "    return folder_counts\n",
        "\n",
        "training_path = 'splitted_dataset/train'\n",
        "validation_path = 'splitted_dataset/val'\n",
        "test_path = 'splitted_dataset/test'\n",
        "\n",
        "training_counts = count_images_in_folders(training_path)\n",
        "validation_counts = count_images_in_folders(validation_path)\n",
        "test_counts = count_images_in_folders(test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Cálculo de Pesos de Clases"
      ],
      "metadata": {
        "id": "YjcdJqgTyVe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "# Definir clases\n",
        "classes = np.array([0, 1])  # 0 = Benigno, 1 = Maligno\n",
        "\n",
        "\n",
        "num_benign = training_counts[\"clasificadas_benigno\"]\n",
        "num_malignant = training_counts[\"clasificadas_maligno\"]\n",
        "\n",
        "# Calcular los pesos de cada clase\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes,\n",
        "                                     y=np.concatenate([np.zeros(num_benign), np.ones(num_malignant)]))\n",
        "\n",
        "# Convertir a diccionario\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
        "\n",
        "print(\"Pesos de clase:\", class_weights)\n",
        "\n",
        "print(\"Training set image counts:\")\n",
        "for folder, count in training_counts.items():\n",
        "    print(f\"{folder}: {count} images\")\n",
        "\n",
        "print(\"\\nValidation set image counts:\")\n",
        "for folder, count in validation_counts.items():\n",
        "    print(f\"{folder}: {count} images\")\n",
        "\n",
        "print(\"\\nTest set image counts:\")\n",
        "for folder, count in test_counts.items():\n",
        "    print(f\"{folder}: {count} images\")"
      ],
      "metadata": {
        "id": "fO8R8YrSyVHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Generadores de Imágenes"
      ],
      "metadata": {
        "id": "Yilj0BqyyfbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRrBm8O83F7a"
      },
      "outputs": [],
      "source": [
        "# Generadores de imágenes\n",
        "\n",
        "def preprocess(images):\n",
        "      # Using the preprocess function of the selected model\n",
        "      # To ensure the new data is in the same format as the original data the model was trained on\n",
        "      return preprocess_input(images)\n",
        "\n",
        "seed=123\n",
        "\n",
        "train_datagen = ImageDataGenerator(fill_mode='wrap',\n",
        "                                    preprocessing_function=preprocess)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    training_path,\n",
        "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    validation_path,\n",
        "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    fill_mode='wrap',\n",
        "    preprocessing_function=preprocess\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_path,  # ← asegúrate de definir esta variable correctamente\n",
        "    target_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False  # Muy importante para evaluar correctamente\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjAgCYu0ztEb"
      },
      "source": [
        "#12. Compilación y Entrenamiento del Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpkX7l7qzvjW",
        "outputId": "50d4ce79-f2ab-4886-bb29-5fcba51fbcf1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9238s\u001b[0m 7s/step - accuracy: 0.8913 - auc: 0.9084 - loss: 0.2593 - precision: 0.6928 - recall: 0.5522 - val_accuracy: 0.9097 - val_auc: 0.9414 - val_loss: 0.2107 - val_precision: 0.7396 - val_recall: 0.6568\n",
            "Epoch 2/10\n",
            "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.9128 - auc: 0.9433 - loss: 0.2069 - precision: 0.7692 - recall: 0.6383"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "\n",
        "# EarlyStopping: Detener el entrenamiento si no hay mejora en la precisión de validación\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy',  # Monitorear la precisión de validación\n",
        "                               patience=5,             # Número de épocas sin mejora antes de detener\n",
        "                               restore_best_weights=True)  # Restaurar los mejores pesos\n",
        "\n",
        "# CSVLogger: Guardar el historial de entrenamiento en un archivo CSV\n",
        "csv_logger = CSVLogger('/content/drive/My Drive/trained_models/training_log.csv', append=True)\n",
        "\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', AUC(), Precision(), Recall()])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping, csv_logger]  # Agrega los callbacks aquí\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVTi1Isnz4Ne"
      },
      "source": [
        "#13. Búsqueda de Hiperparámetros con Keras Tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IslKL9Hez6hH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from keras_tuner import HyperModel\n",
        "\n",
        "class HyperResNet(HyperModel):\n",
        "    def build(self, hp):\n",
        "        base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "        x = base_model.output\n",
        "        x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "        # Hiperparámetro: número de unidades en la capa densa\n",
        "        units = hp.Int('dense_units', min_value=512, max_value=2048, step=512)\n",
        "        x = Dense(units, activation='relu')(x)\n",
        "\n",
        "        # Hiperparámetro: aplicar BatchNormalization o no\n",
        "        if hp.Boolean('batch_norm'):\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        # Hiperparámetro: tasa de Dropout\n",
        "        dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        # Hiperparámetro: learning rate\n",
        "        lr = hp.Float('lr', min_value=1e-5, max_value=1e-3, sampling='LOG')\n",
        "\n",
        "        model = Model(inputs=base_model.input, outputs=x)\n",
        "        model.compile(optimizer=Adam(learning_rate=lr),\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "\n",
        "tuner = RandomSearch(HyperResNet(), objective='val_accuracy', max_trials=5)\n",
        "tuner.search(train_generator, epochs=10, validation_data=validation_generator)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17ysrONKz85R"
      },
      "source": [
        "#14. Guardado de Métricas y Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ld2eEoSx7_8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convertir el historial de métricas en un DataFrame de pandas\n",
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Guardar las métricas en un archivo CSV en tu Google Drive\n",
        "history_csv_path = '/content/drive/My Drive/trained_models/resnet50_full_metrics.csv'\n",
        "history_df.to_csv(history_csv_path, index=False)\n",
        "print(f\"Métricas completas guardadas en: {history_csv_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Evaluación y Matriz de Confusión"
      ],
      "metadata": {
        "id": "VfC4W1p8yvmU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yoJ8Lcg0X5V"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "# Predecir en el conjunto de test\n",
        "y_true = test_generator.classes  # Las verdaderas etiquetas\n",
        "y_pred = model.predict(test_generator)  # Las predicciones del modelo\n",
        "\n",
        "# Convertir las probabilidades a clases (0 o 1) usando un umbral de 0.5\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Calcular la matriz de confusión\n",
        "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "# Guardar la matriz de confusión en un archivo CSV\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix, columns=['Predicted Benign', 'Predicted Malignant'],\n",
        "                              index=['True Benign', 'True Malignant'])\n",
        "\n",
        "conf_matrix_path = '/content/drive/My Drive/trained_models/confusion_matrix.csv'\n",
        "conf_matrix_df.to_csv(conf_matrix_path)\n",
        "print(f\"Matriz de confusión guardada en: {conf_matrix_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. Visualización del Progreso de Entrenamiento"
      ],
      "metadata": {
        "id": "IB4HBr1tyxu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqu5HdcEz9Qp"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17. Guardado del Modelo y del Historial"
      ],
      "metadata": {
        "id": "oiXpbwL9yzvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4DZwFdZAyzrt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL32DmwFoRnX"
      },
      "outputs": [],
      "source": [
        "# Ruta donde quieres guardar el modelo en tu Google Drive\n",
        "model_save_path = '/content/drive/My Drive/trained_models/resnet50_albumentations.h5'\n",
        "\n",
        "# Guarda el modelo completo (estructura y pesos)\n",
        "model.save(model_save_path)\n",
        "print(f\"Modelo guardado en: {model_save_path}\")\n",
        "# Convertir el historial a un DataFrame de pandas\n",
        "hist_df = pd.DataFrame(history.history)\n",
        "\n",
        "# Guardar como CSV en Drive\n",
        "hist_csv_path = '/content/drive/My Drive/trained_models/resnet50_albumentations_history.csv'\n",
        "hist_df.to_csv(hist_csv_path, index=False)\n",
        "print(f\"Historial de entrenamiento guardado en: {hist_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Evaluación Final del Modelo"
      ],
      "metadata": {
        "id": "HaQLQIOxy5Fv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9MEMpiRoTOj"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo en el conjunto de test\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}, Test loss: {test_loss:.4f}\")\n",
        "\n",
        "# Guardar resultados en CSV\n",
        "test_results_df = pd.DataFrame({'Test Accuracy': [test_accuracy], 'Test Loss': [test_loss]})\n",
        "test_results_path = '/content/drive/My Drive/trained_models/resnet50_test_results.csv'\n",
        "test_results_df.to_csv(test_results_path, index=False)\n",
        "print(f\"Resultados de test guardados en: {test_results_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#19. Mejores Hiperparámetros del Tuner"
      ],
      "metadata": {
        "id": "HB2NISKgy7w2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Np0338woU75"
      },
      "outputs": [],
      "source": [
        "# Obtener el mejor modelo y los mejores hiperparámetros\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Crear un diccionario de los mejores hiperparámetros\n",
        "best_hps_values = {param: best_hps.get(param) for param in best_hps.values}\n",
        "\n",
        "# Guardar los hiperparámetros óptimos como un archivo CSV\n",
        "hparams_df = pd.DataFrame([best_hps_values])\n",
        "hparams_path = '/content/drive/My Drive/trained_models/best_hyperparams_resnet50.csv'\n",
        "hparams_df.to_csv(hparams_path, index=False)\n",
        "\n",
        "# Confirmar que se guardaron los hiperparámetros\n",
        "print(f\"Hiperparámetros óptimos guardados en: {hparams_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMk2nsvZMEVdIyVu/CUAQcr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}